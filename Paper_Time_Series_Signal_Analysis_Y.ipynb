{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import ticker\n",
    "import cartopy\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import scipy\n",
    "from scipy import signal\n",
    "from scipy.optimize import curve_fit\n",
    "import math\n",
    "import statistics \n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from numpy import linspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2879\n"
     ]
    }
   ],
   "source": [
    "#R2 = np.load('/fast/gmooers/RG_Paper_Data/15_Min_Interval_Heat.npy')\n",
    "R2 = np.load('/fast/gmooers/Real_Geography_Manuscript/Data_For_Paper/April_SPCAM5_15_Min_Interval_Heat.npy')\n",
    "thresh = 0\n",
    "super_threshold_indices = R2 < thresh\n",
    "R2[super_threshold_indices] = 0\n",
    "thresh = 1\n",
    "super_threshold_indices = R2 > thresh\n",
    "R2[super_threshold_indices] = 1\n",
    "\n",
    "Tropical_R = R2[:,40:55,:]\n",
    "Mid_R = R2[:,64:80,:]\n",
    "\n",
    "#path_to_file = '/fast/gmooers/Preprocessed_Data/7_Years_Spaced/full_physics_essentials_valid_month02_targets.nc'\n",
    "path_to_file = '/fast/gmooers/Real_Geography_Manuscript/Preprocessed_Data/One_Month_Apr/full_physics_essentials_valid_month02_targets.nc'\n",
    "extra_variables = xr.open_dataset(path_to_file)\n",
    "truths = extra_variables.targets[:,:30].values\n",
    "\n",
    "#path_to_file = '/fast/gmooers/Models/Jordan_Best.nc'\n",
    "path_to_file = '/fast/gmooers/Real_Geography_Manuscript/Models/Good_Apr.nc'\n",
    "extra_variables = xr.open_dataset(path_to_file)\n",
    "predictions = extra_variables.Prediction[:,:30].values\n",
    "\n",
    "x = 144\n",
    "y = 96\n",
    "segment = x*y\n",
    "z = 30\n",
    "t = int(len(truths)/(x*y))\n",
    "print(t)\n",
    "reconstructed_targets = np.zeros(shape=(x, y, t, z))\n",
    "reconstructed_features = np.zeros(shape=(x, y, t, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 96, 2879, 30)\n",
      "(144, 96, 2879, 30)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(t):\n",
    "    for j in range(y):\n",
    "        for k in range(x):\n",
    "            A = truths[count,:]\n",
    "            B = predictions[count,:]\n",
    "            reconstructed_targets[k, j, i, :] = A\n",
    "            reconstructed_features[k, j, i, :] = B\n",
    "            count = count+1\n",
    "            \n",
    "print(reconstructed_targets.shape)\n",
    "print(reconstructed_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it\n"
     ]
    }
   ],
   "source": [
    "others = netCDF4.Dataset(\"/fast/gmooers/Raw_Data/extras/TimestepOutput_Neuralnet_SPCAM_216.cam.h1.2009-01-01-72000.nc\")\n",
    "plev = np.array(others.variables['lev'])\n",
    "ps = np.array(others.variables['PS'])\n",
    "g = 9.81 #m/s^2\n",
    "#print(plev)\n",
    "hyai = np.array(others.variables['hyai'])\n",
    "hybi = np.array(others.variables['hybi'])\n",
    "#print(hyai.shape)\n",
    "#print(hyai)\n",
    "cp = 1004.0\n",
    "PS = 1e5\n",
    "P0 = 1e5\n",
    "P = P0*hyai+PS*hybi # Total pressure [Pa]\n",
    "dp = P[1:]-P[:-1] # Differential pressure [Pa]\n",
    "#convert from k/s to w/m^2\n",
    "reconstructed_targets = reconstructed_targets*dp[None, None, None, :]*cp/g\n",
    "reconstructed_features = reconstructed_features*dp[None, None, None, :]*cp/g\n",
    "print('made it')\n",
    "\n",
    "lat_list = np.linspace(-90.0, 90.0, 96)\n",
    "lon_list = np.linspace(0.0, 357.5, 144)\n",
    "\n",
    "#method 1 - Hovmueller Github \n",
    "a = 6.37e6 #radius of the earth\n",
    "dlat = np.abs(lat_list[1]-lat_list[0])\n",
    "dlon = np.abs(lon_list[0]-lon_list[1])\n",
    "gridCellWidth = (dlon*np.pi/180.)*a*np.cos(lat_list*np.pi/180.)\n",
    "reconstructed_targets = reconstructed_targets*gridCellWidth[None,:,None,None]\n",
    "reconstructed_features = reconstructed_features*gridCellWidth[None,:,None,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to analyse original time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(true, guess, name):\n",
    "    plt.plot(true, label = 'Truth')\n",
    "    plt.plot(guess, label = 'Prediction')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Days of the Year', fontsize = 20)\n",
    "    plt.ylabel('Heat Rate in K/s', fontsize = 20)\n",
    "    #plt.ylim(bottom, top)\n",
    "    plt.title('Time series for '+name+' location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get e folding time scale from reconstructed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Signal_Test(point_a, point_b, labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(labels)\n",
    "    tcoor = np.arange(0, len(point_b), 1)/96\n",
    "    plt.plot(tcoor, point_a, label=\"Truth\", color = 'blue')\n",
    "    plt.plot(tcoor, point_b, label = \"Prediction\", color = 'green')\n",
    "    it = np.argwhere(np.diff(np.sign(point_a/point_a[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    plt.scatter(tau, np.exp(-1)*point_a[0], s=100, marker = 'H', color = 'orange', label=\"Truth e-folding Distance: \"+str(round(tau[0],2))+ ' days')\n",
    "    it = np.argwhere(np.diff(np.sign(point_b/point_b[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    plt.scatter(tau, np.exp(-1)*point_b[0], s=100, marker = 'H', color = 'red', label=\"Predicted e-folding Distance: \"+str(round(tau[0],2))+ ' days')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time in Days\", fontsize = 15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to reconstruct and deconstruct time series for anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AutoCorr_Point(a):\n",
    "    a = np.concatenate((a,np.zeros(len(a)-1)))\n",
    "    A = np.fft.fft(a)\n",
    "    S = np.conj(A)*A\n",
    "    c_fourier = np.fft.ifft(S)\n",
    "    c_fourier = c_fourier[:(c_fourier.size//2)+1]\n",
    "    return c_fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_generator_x(truths, preds):\n",
    "    corr_truth = np.zeros(shape=(x,y,t,z))\n",
    "    corr_truth[:,:,:,:] = np.nan\n",
    "    corr_pred = np.zeros(shape=(x,y,t,z))\n",
    "    corr_pred[:,:,:,:] = np.nan\n",
    "    for i in range(len(corr_truth[0][0])):\n",
    "        for j in range(len(corr_truth[0])):\n",
    "            for k in range(len(corr_truth[0][0][0])):\n",
    "                truth_fourier = AutoCorr_Point(truths[:,j,i,k])\n",
    "                pred_fourier = AutoCorr_Point(preds[:,j,i,k])\n",
    "                corr_truth[:,j,i,k] = truth_fourier/np.max(truth_fourier)\n",
    "                corr_pred[:,j,i,k] = pred_fourier/np.max(pred_fourier)\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "    \n",
    "    return corr_truth, corr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_generator_y(truths, preds):\n",
    "    corr_truth = np.zeros(shape=(x,y,t,z))\n",
    "    corr_truth[:,:,:,:] = np.nan\n",
    "    corr_pred = np.zeros(shape=(x,y,t,z))\n",
    "    corr_pred[:,:,:,:] = np.nan\n",
    "    for i in range(len(corr_truth)):\n",
    "        for j in range(len(corr_truth[0][0])):\n",
    "            for k in range(len(corr_truth[0][0][0])):\n",
    "                truth_fourier = AutoCorr_Point(truths[i,:,j,k])\n",
    "                pred_fourier = AutoCorr_Point(preds[i,:,j,k])\n",
    "                corr_truth[i,:,j,k] = truth_fourier/np.max(truth_fourier)\n",
    "                corr_pred[i,:,j,k] = pred_fourier/np.max(pred_fourier)\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "    \n",
    "    return corr_truth, corr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/gmooers/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/export/home/gmooers/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n"
     ]
    }
   ],
   "source": [
    "x_corr_truth, x_corr_pred = field_generator_x(reconstructed_targets, reconstructed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home/gmooers/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/export/home/gmooers/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n"
     ]
    }
   ],
   "source": [
    "y_corr_truth, y_corr_pred = field_generator_y(reconstructed_targets, reconstructed_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 96, 2879, 30)\n",
      "(144, 96, 2879, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_corr_truth.shape)\n",
    "print(y_corr_truth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break into regions, and break down further by skill score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tropical_truths = np.squeeze(corr_truth[:, 40:55, :,:])\n",
    "#tropical_preds = np.squeeze(corr_pred[:, 40:55, :,:])\n",
    "\n",
    "#midlat_truths = np.squeeze(corr_truth[:, 64:80, :,:])\n",
    "#midlat_preds = np.squeeze(corr_pred[:, 64:80, :,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_array_generator(data_array, R_array):\n",
    "    poor_output_array = np.zeros(shape=(x, len(data_array[0]),t, z))\n",
    "    poor_output_array[:,:,:,:] = np.nan\n",
    "    good_output_array = np.zeros(shape=(x, len(data_array[0]),t, z))\n",
    "    good_output_array[:,:,:,:] = np.nan\n",
    "    good_count = 0\n",
    "    bad_count = 0\n",
    "    for i in range(len(data_array)):\n",
    "        for j in range(len(data_array[i])):\n",
    "            for k in range(len(data_array[i][j][0])):\n",
    "                if R_array[i,j,k] >= 0.7:\n",
    "                    good_output_array[i,j,:,k]=data_array[i,j,:,k]\n",
    "                    if k == len(data_array[i][j][0]) -1:\n",
    "                        good_count = good_count+1\n",
    "                \n",
    "                if R_array[i,j,k] < 0.3:\n",
    "                    poor_output_array[i,j,:,k]=data_array[i,j,:,k]\n",
    "                    if k == len(data_array[i][j][0]) -1:\n",
    "                        bad_count = bad_count+1\n",
    "     \n",
    "    print(\"Skilled Count is:\", good_count, \"Unskilled Count is:\", bad_count, \"Total_Count is:\", 144*len(data_array[0]))\n",
    "    return good_output_array, poor_output_array  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144, 96, 30)\n"
     ]
    }
   ],
   "source": [
    "print(R2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skilled Count is: 7343 Unskilled Count is: 1567 Total_Count is: 13824\n",
      "Skilled Count is: 7343 Unskilled Count is: 1567 Total_Count is: 13824\n",
      "Skilled Count is: 7343 Unskilled Count is: 1567 Total_Count is: 13824\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.90 GiB for an array with shape (144, 96, 2879, 30) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c84bc8d6d269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_all_good_truths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all_bad_truths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskill_array_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_corr_truth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_all_good_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_all_bad_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskill_array_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_corr_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-78c9533088eb>\u001b[0m in \u001b[0;36mskill_array_generator\u001b[0;34m(data_array, R_array)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpoor_output_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpoor_output_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgood_output_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgood_output_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgood_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 8.90 GiB for an array with shape (144, 96, 2879, 30) and data type float64"
     ]
    }
   ],
   "source": [
    "x_all_good_truths, x_all_bad_truths = skill_array_generator(x_corr_truth, np.squeeze(R2))\n",
    "x_all_good_preds, x_all_bad_preds = skill_array_generator(x_corr_pred, np.squeeze(R2))\n",
    "\n",
    "y_all_good_truths, y_all_bad_truths = skill_array_generator(x_corr_truth, np.squeeze(R2))\n",
    "y_all_good_preds, y_all_bad_preds = skill_array_generator(x_corr_pred, np.squeeze(R2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_surface =np.squeeze(np.squeeze(R2)[:,:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo(feats, name):\n",
    "    f = feats.ravel()\n",
    "    num_bins = 100\n",
    "    #change bin specification\n",
    "    hist, bins = np.histogram(f, num_bins, normed=True)\n",
    "    bin_centers = (bins[1:]+bins[:-1])*0.5\n",
    "    plt.plot(bin_centers, hist, label = 'R2 Skill')\n",
    "    plt.legend(loc='best')\n",
    "    #plt.xlim(0)\n",
    "    plt.xlabel('Skill Score', fontsize = 15)\n",
    "    plt.ylabel('Frequency', fontsize = 15)\n",
    "    plt.title(name, fontsize = 15)\n",
    "    #plt.yscale('log')\n",
    "    #plt.xscale('log')\n",
    "    #plt.savefig('Figures/Original_Precip_Histogram_1000.png')\n",
    "    plt.show()\n",
    "\n",
    "title = \"Histogram of R2\"\n",
    "#histo(R_surface, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_pdf(values, titles):\n",
    "    R_ravel = values.ravel()\n",
    "    # this create the kernel, given an array it will estimate the probability over that values\n",
    "    kde = gaussian_kde(R_ravel)\n",
    "    # these are the values over wich your kernel will be evaluated\n",
    "    dist_space = linspace( np.min(R_ravel), np.max(R_ravel), 100)\n",
    "    plt.plot( dist_space, kde(dist_space) )\n",
    "    plt.xlabel(\"R2 Skill Score\")\n",
    "    plt.ylabel(\"Probability Density\")\n",
    "    plt.title(titles)\n",
    "\n",
    "title = \"R2 PDF\"\n",
    "#new_pdf(R_surface, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_gen(feat, names):\n",
    "    maxi = 10.0\n",
    "    bins = 100\n",
    "    freq, edges = np.histogram(feat, bins, density=True)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    plt.plot(edges[:-1], freq,  label = \"R2\", alpha = 0.5, color = 'blue')\n",
    "    plt.xlabel('Skill Score', fontsize = 15)\n",
    "    plt.ylabel('Probability', fontsize = 15)\n",
    "    plt.title('R2 PDF', fontsize = 15)\n",
    "    plt.legend(loc = 'best')\n",
    "    #plt.xscale('log')\n",
    "    #plt.yscale('log')\n",
    "\n",
    "title = \"R2 PDF\"    \n",
    "pdf_gen(R_surface, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaged Points to Single Time Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a single plot for the global surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_truth = np.squeeze(corr_truth[:,:,:,-1])\n",
    "global_truth = np.nanmean(global_truth, axis = 0)\n",
    "global_truth = np.nanmean(global_truth, axis = 0)\n",
    "\n",
    "global_pred = np.squeeze(corr_pred[:,:,:,-1])\n",
    "global_pred = np.nanmean(global_pred, axis = 0)\n",
    "global_pred = np.nanmean(global_pred, axis = 0)\n",
    "\n",
    "title = 'Global Surface Average'\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test(truth_test, pred_test, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Signal_Test_Comp_Truth(point_a, point_b, point_c, title_a, title_b, title_c, title):\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.title(title)\n",
    "    tcoor = np.arange(0, len(point_b), 1)/96\n",
    "    plt.plot(tcoor, point_a, label=\"All\", color = 'blue')\n",
    "    plt.plot(tcoor, point_b, label = \"UnSkillful\", color = 'green')\n",
    "    plt.plot(tcoor, point_c, label = \"Skillful\", color = 'purple')\n",
    "    \n",
    "    it = np.argwhere(np.diff(np.sign(point_a/point_a[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    plt.scatter(tau, np.exp(-1)*point_a[0], s=100, marker = 'H', color = 'blue', label=title_a+\": \"+str(round(tau[0],2))+' days')\n",
    "    \n",
    "    it = np.argwhere(np.diff(np.sign(point_b/point_b[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    plt.scatter(tau, np.exp(-1)*point_b[0], s=100, marker = 'H', color = 'green', label=title_b+\": \"+str(round(tau[0],2))+' days')\n",
    "    \n",
    "    it = np.argwhere(np.diff(np.sign(point_c/point_c[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    plt.scatter(tau, np.exp(-1)*point_c[0], s=100, marker = 'H', color = 'purple', label=title_c+\": \"+str(round(tau[0],2))+' days')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time in Days\", fontsize = 15)\n",
    "    plt.xlim(0, 0.35)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(midlat_truths[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.squeeze(bad_midlats_truths[:,:,:,-1])\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.squeeze(good_midlats_truths[:,:,:,-1])\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Mid Latitude Comparison'\n",
    "regions = \"Mid Lats\"\n",
    "title_1 = 'All R'\n",
    "title_2 = 'R < 0.3' \n",
    "title_3 = 'R > 0.7'\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(tropical_truths[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.squeeze(bad_tropical_truths[:,:,:,-1])\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.squeeze(good_tropical_truths[:,:,:,-1])\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Tropical Comparison'\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(corr_truth[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.squeeze(all_bad_truths[:,:,:,-1])\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.squeeze(all_good_truths[:,:,:,-1])\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Global Surface Comparison'\n",
    "regions = \" Global\"\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_gen(data_array, R_array):\n",
    "    poor_output_array = np.zeros(shape=(x, len(data_array[0]),t))\n",
    "    poor_output_array[:,:,:] = np.nan\n",
    "    good_output_array = np.zeros(shape=(x, len(data_array[0]),t))\n",
    "    good_output_array[:,:,:] = np.nan\n",
    "    ten = np.nanpercentile(R_array, 10)\n",
    "    ninty = np.nanpercentile(R_array, 90)\n",
    "    good_count = 0\n",
    "    bad_count = 0\n",
    "    for i in range(len(data_array)):\n",
    "        for j in range(len(data_array[i])):\n",
    "                if R_array[i,j] >= ninty:\n",
    "                    good_output_array[i,j,:]=data_array[i,j,:]\n",
    "                    good_count = good_count+1\n",
    "                \n",
    "                if R_array[i,j] <= ten:\n",
    "                    poor_output_array[i,j,:]=data_array[i,j,:]\n",
    "                    bad_count = bad_count+1\n",
    "     \n",
    "    print(\"Skilled Count is:\", good_count, \"Unskilled Count is:\", bad_count, \"Total_Count is:\", 144*len(data_array[0]))\n",
    "    return good_output_array, poor_output_array\n",
    "\n",
    "good_tropical_truths, bad_tropical_truths = percentile_gen(np.squeeze(tropical_truths[:,:,:,-1]), np.squeeze(np.squeeze(Tropical_R)[:,:,-1]))\n",
    "good_tropical_preds, bad_tropical_preds = percentile_gen(np.squeeze(tropical_preds[:,:,:,-1]), np.squeeze(np.squeeze(Tropical_R)[:,:,-1]))\n",
    "good_midlats_truths, bad_midlats_truths = percentile_gen(np.squeeze(midlat_truths[:,:,:,-1]),np.squeeze(np.squeeze(Mid_R)[:,:,-1]))\n",
    "good_midlats_preds, bad_midlats_preds = percentile_gen(np.squeeze(midlat_preds[:,:,:,-1]), np.squeeze(np.squeeze(Mid_R)[:,:,-1]))\n",
    "all_good_truths, all_bad_truths = percentile_gen(np.squeeze(corr_truth[:,:,:,-1]), np.squeeze(np.squeeze(R2)[:,:,-1]))\n",
    "all_good_preds, all_bad_preds = percentile_gen(np.squeeze(corr_pred[:,:,:,-1]), np.squeeze(np.squeeze(R2)[:,:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(midlat_truths[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.nanmean(bad_midlats_truths, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.nanmean(good_midlats_truths, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Mid Latitude Comparison'\n",
    "regions = \"Mid Lats\"\n",
    "title_1 = 'All R'\n",
    "title_2 = 'Bottom 10%' \n",
    "title_3 = 'Top 10%'\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(tropical_truths[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.nanmean(bad_tropical_truths, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.nanmean(good_tropical_truths, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Tropical Comparison'\n",
    "regions = \"Tropics\"\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = np.squeeze(corr_truth[:,:,:,-1])\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "all_points = np.nanmean(all_points, axis = 0)\n",
    "\n",
    "bad_points = np.nanmean(all_bad_truths, axis = 0)\n",
    "bad_points = np.nanmean(bad_points, axis = 0)\n",
    "\n",
    "good_points = np.nanmean(all_good_truths, axis = 0)\n",
    "good_points = np.nanmean(good_points, axis = 0)\n",
    "\n",
    "titles = 'Global Comparison'\n",
    "regions = \"Global Surface\"\n",
    "truth_test = AutoCorr_Point(global_truth)\n",
    "pred_test = AutoCorr_Point(global_pred)\n",
    "Signal_Test_Comp_Truth(all_points/np.max(all_points), bad_points/np.max(bad_points), good_points/np.max(good_points), title_1, title_2, title_3, titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_gen(datas):\n",
    "    q1 = []\n",
    "    median = []\n",
    "    q3 = []\n",
    "    for i in range(len(datas[0][0])):\n",
    "        temp = np.squeeze(datas[:,:,i])\n",
    "        temp = temp.ravel()\n",
    "        q1.append(np.nanpercentile(temp, 25))\n",
    "        median.append(np.nanpercentile(temp, 50))\n",
    "        q3.append(np.nanpercentile(temp, 75))\n",
    "    \n",
    "    q1 = np.array(q1)\n",
    "    median = np.array(median)\n",
    "    q3 = np.array(q3)\n",
    "    \n",
    "    return q1/np.max(median), median/np.max(median), q3/np.max(median)\n",
    "\n",
    "def Sig_Test(point_b):\n",
    "    tcoor = np.arange(0, len(point_b), 1)/96\n",
    "    it = np.argwhere(np.diff(np.sign(point_b/point_b[0]-np.exp(-1))))[0]\n",
    "    tau = tcoor[it]\n",
    "    return tau[0]\n",
    "\n",
    "def e_time_getter(data_array):\n",
    "    e_times = np.zeros(shape = (x, y))\n",
    "    e_times[:,:] = np.nan\n",
    "    for i in range(len(data_array)):\n",
    "        for j in range(len(data_array[i])):\n",
    "            temp = np.squeeze(data_array[i,j,:])\n",
    "            value =  Sig_Test(temp)\n",
    "            e_times[i,j] = value\n",
    "            \n",
    "    e_data = e_times.ravel()\n",
    "    filtered_data = e_data[~np.isnan(e_data)]\n",
    "    return filtered_data\n",
    "\n",
    "def plot_stats(all_array, bad_array, good_array, title):\n",
    "    labels = [\"All R\", \"Bottom 10%\", \"Top 10%\"]\n",
    "    names = [\"All\",\"Unskillful\",\"Skillful\"]\n",
    "    colors = [\"purple\",\"red\",\"blue\"]\n",
    "    datas = [all_array, bad_array, good_array]\n",
    "    #filtered_data = e_time_getter(datas[1])\n",
    "    #plt.boxplot(filtered_data)\n",
    "    fig, ax = plt.subplots(figsize = (12,8))\n",
    "    plt.title(title+ \" IQR\")\n",
    "    for i in range(len(labels)):\n",
    "        q1, median, q3 = stats_gen(datas[i])\n",
    "        tcoor = np.arange(0, len(median), 1)/96\n",
    "        if i > 0:\n",
    "            #lower_bound = plt.plot(tcoor, q1, color = colors[i], linestyle='--', dashes=(5, 8), alpha=0.8)\n",
    "            #upper_bound = plt.plot(tcoor, q3, color = colors[i], linestyle='--', dashes=(5, 10), alpha = 0.8)\n",
    "            y = plt.plot(tcoor, median, color = colors[i])\n",
    "            plt.fill_between(tcoor, q1,  q3, alpha = 0.2, color = colors[i])\n",
    "            it = np.argwhere(np.diff(np.sign(median/median[0]-np.exp(-1))))[0]\n",
    "            tau = tcoor[it]\n",
    "            plt.scatter(tau, np.exp(-1)*median[0], s=100, marker = 'H', color = colors[i], label=labels[i]+\": \"+str(round(tau[0],2))+' days')\n",
    "    \n",
    "   \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Time in Days\", fontsize = 15)\n",
    "    plt.xlim(0, 0.40)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "plot_stats(np.squeeze(midlat_truths[:,:,:,-1]), bad_midlats_truths, good_midlats_truths, \"Mid Latitudes\")\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Midlat_All.npy\", np.squeeze(midlat_truths[:,:,:,-1]))\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Midlat_bad.npy\", bad_midlats_truths)\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Midlat_good.npy\", good_midlats_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(np.squeeze(tropical_truths[:,:,:,-1]), bad_tropical_truths, good_tropical_truths, \"Tropics\")\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Tropical_All.npy\", np.squeeze(tropical_truths[:,:,:,-1]))\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Tropical_bad.npy\", bad_tropical_truths)\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Tropical_good.npy\", good_tropical_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(np.squeeze(corr_truth[:,:,:,-1]), all_bad_truths, all_good_truths, \"Global_Surface\")\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Surface_All.npy\", np.squeeze(corr_truth[:,:,:,-1]))\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Surface_bad.npy\", all_bad_truths)\n",
    "#np.save(\"/fast/gmooers/RG_Paper_Data/Autocorr_Surface_good.npy\", all_good_truths)\n",
    "print(\"Global surface Preserved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
